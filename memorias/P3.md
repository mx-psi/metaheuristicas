---
title: "\\textbf{Práctica 3.b:} \\ Enfriamiento Simulado, Búsqueda Local Reiterada y Evolución Diferencial para el Problema del Aprendizaje de Pesos en Características"
author: Pablo Baeyens
subtitle: Metaheurísticas
documentclass: scrbook
classoption: oneside
algos: SA, ILS y Evolución Diferencial
lang: es
colorlinks: true
bibliography: assets/citas.bib
biblio-style: apalike
link-citations: true
citation-style: assets/estilo.csl
numbersections: true
toc: true
---

# Descripción de los algoritmos considerados

En todos los algoritmos utilizamos funciones definidas en las prácticas anteriores que indico brevemente:

`obtenUniforme` obtiene un vector de pesos aleatorio:

```haskell
obtenUniforme :: DataSet → Weights
obtenUniforme ds = creaVector (repite (numCaracterísticas ds) (uniforme (0,1)))
```

`creaSol` crea una solución a partir de un vector de pesos y `solAleatoria` obtiene una solución aleatoria:

```haskell
creaSol :: DataSet → Weights → Solution
creaSol ws = Sol {nSol = ++nEvals, evaluación= objetivo ws ds, pesos = ws}

solAleatoria :: DataSet → Solution
solAleatoria ds = creaSol ds (obtenUniforme ds)
```

`veces :: Int → (a → a) → a` aplica un número de veces una función.
Usualmente se llama de la forma:

```haskell
(100 `veces` muta) v -- aplica muta 100 veces al vector v y devuelve el resultado
```

## Enfriamiento simulado

El enfriamiento simulado es una búsqueda basada en trayectorias que se inspira en el proceso de cristalización del acero. En cada paso se enfría la temperatura y se generan vecinos utilizando el operador de mutación de la búsqueda local considerado en la práctica 1, aceptándolos probabilísticamente en función de la temperatura.

Para este algoritmo he implementado una estructura de datos que guarda el estado en cada paso y el sinónimo `Temperature` para `Double`:

```haskell
data SAData = SA {
  getSol   :: Solution,    -- | Solución actual
  getBest  :: Solution,    -- | Mejor solución
  getK     :: Int,         -- | Número de enfriamientos
  getT     :: Temperature, -- | Temperatura actual
  nExitos  :: Int,         -- | Número de éxitos actual
  nVecinos :: Int,         -- | Número de vecinos actual
  beta     :: Double -- | La constante beta
  }
```

He considerado dos versiones del enfriamiento simulado de acuerdo con lo que aparece en el pdf de esta práctica: una versión con esquema de Cauchy modificado y otra con esquema proporcional ya que esta daba mejores resultados (ver [Experimentos y análisis de resultados]). Se diferencian en el esquema de enfriamiento y en el número de vecinos máximos considerado ($10n$ en el primer caso y $5n$ en el segundo). Los esquemas serían:

```haskell
cauchy sa = do
  sa.getT = sa.getT/(1 + sa.beta*sa.getT)
prop sa = do
  sa.getT = 0.95*sa.getT
```

Además defino algunas funciones auxiliares que obtienen algunas constantes e inicializan el estado:

```haskell
getT0 :: Solution → Temperature -- Temperatura inicial
getT0 s = mu*(50*getEval s)/(- log phi)
  where mu  = 0.3 :: Double
        phi = 0.3 :: Double

maxVecinos sa = 10*(numCaracterísticas (getV sa.getSol))
maxExito   s = 0.1*(maxVecinos sa)

getBeta :: Solution → Double → Double
getBeta s t0 = (t0 - tf)/((15000/(maxVecinos s))*t0*tf)
  where mu  = 0.3
        phi = 0.3
        t0 = getT0 s  -- Temperatura inicial
        tf = 10**(-3) -- Temperatura final

inicial :: Solution → SAData -- Solución inicial
inicial s = SA {getSol = s, getBest = s, getK = 0,
             getT = getT0 s, nExitos = -1, nVecinos = 0,
             beta = getBeta s}
```

\newpage

El algoritmo completo tiene el siguiente código repite hasta el número máximo de evaluaciones o la ausencia de éxitos la aplicación de la función `saStep`:

```haskell
enfriamiento :: Algorithm
enfriamiento ds = pesos ((
  until (\sa → nEvals >= 15000 || sa.nExito == 0) 
   (saStep ds) 
   (inicial (solAleatoria ds))
   ).getBest)
```

La función `saStep` reinicia los contadores, explora los vecinos y enfría (`enfria` es `cauchy` o `prop`):

```haskell
saStep :: DataSet → SAData → SAData
saStep ds sa = do
  let sa.nExitos  = 0
  let sa.nVecinos = 0
  let nuevoSa = exploraVecinos ds sa
  return (enfria newSA)
```

Finalmente `exploraVecinos` hace el bucle interior: hasta haber generado el número máximo de vecinos o éxitos `generaVecino`, función en la que generamos un vecino y actualizamos la solución actual con la condición que depende de la temperatura:

```haskell
exploraVecinos :: DataSet → SAData → SAData
exploraVecinos ds sa = until 
  (\sa → sa.nVecinos > maxVecinos sa || sa.nExitos > maxExitos sa) 
  (generaVecino ds) 
  sa

generaVecino :: DataSet → SAData → SAData
generaVecino ds sa = do
  s' <- creaSol (vecino 0.3 v)
  let dif = getEval sa.getSol - getEval s'
  sa.nVecinos += 1
  
  if dif < 0 || rand (0,1) <= exp (-dif/(sa.getK*sa.getT)) then
    sa.getSol  = s'
    sa.getBest = max s' s
    sa.nExitos += 1
  return sa
```

\newpage

## Búsqueda Local Reiterada

La búsqueda local reiterada consiste en la aplicación reiterada del algoritmo de búsqueda local seguido de una mutación brusca. Este algoritmo se construye a partir del algoritmo de búsqueda local implementado en la práctica 1 y del operador de mutación considerados en estas prácticas (el pseudocódigo de estos algoritmos está en la memoria de la práctica 1).

Dentro del pseudocódigo llamamos `local :: DataSet → Solution → Solution` a la búsqueda local (que en este caso hace un máximo de 1000 evaluaciones de la función objetivo) y `vecino :: Weights → Weights` al operador vecino (que en este caso usa $\sigma = 0,4$).

Definimos dos funciones auxiliares. En primer lugar el operador de mutación `muta` que toma una solución y le aplica una mutación en $0,1\cdot n$ características. Para ello usa `generaIndices` que genera los índices distintos y luego va aplicando con `vecinoILS` una mutación en cada uno de ellos:

```haskell
muta :: DataSet → Solution → Rand Solution
muta ds s = do
  let indices = generaIndices v (n/10) 
  let nuevoV  = acumula vecinoILS indices v
  return (creaSol ds newv)
  where v = getV s
        n = length v
```

A continuación un paso del algoritmo. 
En primer lugar reinicia el contador del número de evaluaciones para cada búsqueda local.
Luego muta la mejor solución hasta el momento, le aplica un búsqueda local reducida y finalmente devuelve el mejor (máximo) entre las soluciones generadas:

```haskell
pasoIls :: DataSet → Solution → Rand Solution
pasoIls ds s = do
  let nEvals = 0
  let s'     = muta ds s
  let s''    = local ds s'
  return (max s s'')
```

El algoritmo de búsqueda local reiterada consiste en: obtener una solución aleatoria, aplicar la búsqueda local a esta y a continuación aplicar 14 veces el paso definido anteriormente:

```haskell
ils :: Algorithm
ils ds = pesos (
  14 `veces` (pasoIls ds) 
  (local ds (solAleatoria ds)))
```

## Evolución Diferencial

Los algoritmos de evolución diferencial son un tipo de algoritmos evolutivos.
Para estos algoritmos he utilizado un vector con acceso aleatorio para la población (`DEPopulation`) lugar de conjuntos.
El nombre de las funciones utilizadas es el mismo. 
El esquema general de un algoritmo de evolución diferencial dado un `paso :: DataSet → DEPopulation → DEPopulation` es:

```haskell
difEv :: (DataSet → DEPopulation → DEPopulation) → DataSet → Weights
difEv paso ds = max (hastaQue (nEvals > 15000) (paso ds) (iniciaPop ds tam))
```

A continuación describo el paso de cada algoritmo.

### Versión aleatoria

La función `pasoRand` describe el paso de la versión aleatoria de la evolución diferencial.
En primer lugar toma un índice aleatorio `jrand` que siempre se modificará.
A continuación aplica la operación de mutación `mutaRand` a cada elemento de la población,
posiblemente sustituyendo la solución por la mutada si es mejor:

```haskell
pasoRand :: DataSet → DEPopulation → DEPopulation
pasoRand ds pop = do
  let jrand = rand (0, nFeats ds - 1)
  map (mutaRand ds pop jrand) pop
```

La función `mutaRand` muta cada elemento `s`.
En primer lugar toma los 3 `vecinos` que se utilizan en la operación de cruce mediante `ndistintosDe` que toma, en este caso, 3 elementos distintos de `s` de `pop`.
A continuación crea la nueva solución aplicando a cada gen de la solución la operación de cruce `cruceRand`.
Finalmente devuelve la mejor solución de entre la mutada y la original.

```haskell
mutaRand :: DataSet → DEPopulation → Int → Solution → Solution
mutaRand ds pop jrand s = do
  let vecinos = map getV (nDistintosDe pop s 3) :: [Weights]
  let newV    = imap (cruceRand vecinos jrand) (getV s) :: Weights
  return (max (creaSol ds newV) s)
```

Finalmente la operación `cruceRand` hace el cruce coordenada a coordenada.
Si el número generado es menor que `cr` o si estamos en el gen `jrand` aplicamos la operación de mutación tomando los elementos en la posición `i` de cada vecino.
En otro caso mantenemos la coordenada.

```haskell
cruceRand :: [Weights] → Int → Int → Double → Double
cruceRand vecinos jrand i xi = do
  let [a,b,c] = map (\v → v[i]) vecinos :: [Double]

  if rand (0,1) < cr || i == jrand then
    return (min 1 (max 0 (a + f*(b-c))))
  else
    return xi
  where cr = 0.5 :: Double
        f  = 0.5 :: Double
```

### Versión current-to-best

La versión current-to-best es muy similar pero tomamos el mejor de la población y lo pasamos hasta la operación de cruce. La operación de cruce es `mutaBest` que ahora toma el mejor de la población `best`.

```haskell
pasoBest :: DataSet → DEPopulation → DEPopulation
pasoBest ds pop = do
  let jrand = rand (0, nFeats ds - 1)
  let best  = max pop
  imap (mutaBest ds pop jrand best) pop
```

La operación de mutación ahora toma dos elementos distintos en `vecinos`, aplica el cruce `cruceBest`
y actualiza la solución si ha mejorado:

```haskell
mutaBest :: DataSet → DEPopulation → Int → Solution → Solution → Solution
mutaBest ds pop jrand best s = do
  let vecinos = map getV (nDistintosDe pop s 2) :: [Weights]
  let newV =  imap (cruceBest vecinos jrand best) (getV s) :: Weights
  return (max (creaSol ds newV) s)
```

`cruceBest` hace el cruce coordenada a coordenada.
Si el número generado es menor que `cr` o si estamos en el gen `jrand` aplicamos la operación de mutación tomando los elementos en la posición `i` de cada vecino y de la mejor solución.
En otro caso mantenemos la coordenada.

```haskell
cruceBest :: [Weights] → Int → Solution → Int → Double → Double
cruceBest vecinos jrand best i xi = do
  let [a,b] = map (\v → v[i]) vecinos
  let xbest = best[i] :: Double

  if rand (0,1) < cr || i == jrand then
    return (min 1 (max 0 (xi + f*(xbest-xi) + f*(a-b))))
  else
    return xi
  where cr = 0.5 :: Double
        f  = 0.5 :: Double
```

# Procedimiento considerado para desarrollar la práctica

La implementación de la práctica se ha realizado en el lenguaje de programación funcional puro [**Haskell**](https://www.haskell.org/).
No se ha utilizado ningún framework de metaheurísticas.

El código se incluye en la carpeta asociada. Para compilarlo es necesario instalar `stack` y *LLVM* en su versión 3.9.
La instalación de `stack` puede hacerse siguiendo las instrucciones que aparecen [en su página web](https://docs.haskellstack.org/).
`stack` se encarga de descargar los paquetes necesarios para la compilación del programa.

Para el SO que he utilizado para desarrollar la práctica (Linux Mint) *LLVM* puede instalarse con el paquete `llvm-3.9`. 
Es importante añadir al `PATH` la carpeta `/usr/lib/llvm-3.9/bin/`.
Puede consultarse la instalación de LLVM para otras plataformas [en su página web](https://llvm.org/).
Opcionalmente puede no instalarse `llvm` (con lo que el programa se compilará vía `gcc`) eliminando la flag `-fllvm` de `ghc-options` en el fichero `mhP1.cabal`.

Se incluye un Makefile para facilitar la ejecución. 
Para compilar el programa con `stack` y ejecutarlo sin argumentos hacemos:

```
make build
make run
```

La primera ejecución de `stack` necesita descargar el compilador de Haskell y los paquetes necesarios para satisfacer las dependencias por lo que es probable que tarde un tiempo.

El programa admite 3 modos de funcionamiento:

- Si no se pasa ningún argumento (`make run`) el programa ejecuta todos los algoritmos para todos los ficheros que estén en la carpeta `Data` con un generador de números aleatorio obtenido del sistema.
- Si se le pasa un único argumento lee un generador de ese argumento y ejecuta todos los algoritmos para todos los ficheros que estén en la carpeta `Data` con ese generador.
- Si se le pasa más de un argumento lee un generador del primer argumento y a continuación lee una lista de ficheros. Ejecuta todos los algoritmos sobre esos ficheros con el generador dado como argumento. 

\newpage

Para los dos últimos modos ejecutamos directamente:

```bash
./mhP3 generador [ficheros]
```

El generador utilizado para una ejecución en la que no se provee de uno se imprime al comenzar la ejecución. 
Para pasarlo como argumento debe pasarse entre comillas.

El programa **asume que los saltos de línea son de tipo UNIX** (es decir, los finales de línea deben ser `\n` y no `\r\n`) y elimina datos duplicados de los ficheros de entrada.
Se incluye una carpeta `Data` con los ficheros de entrada de los datasets a analizar en la práctica con saltos de línea tipo UNIX.

El código está disponible en la carpeta `FUENTES` y se divide en las siguientes partes:

- `Main.hs` y `Base.hs` define el bucle principal de ejecución del programa y los tipos básicos
- `Read.hs` implementa la lectura de archivos tipo arff.
- `KNN.hs` implementa el algoritmo de vecino más cercano y las funciones de evaluación de un vector de pesos (*leave one out*, tasa de clasificación, tasa de simplicidad...).
- `P1.hs` implementa los algoritmos de la práctica 1 (RELIEF y BL)
- `Genetic.hs` implementa los algoritmos genéticos estacionarios y generacionales así como funciones auxiliares asociadas
- `Memetic.hs` implementa los algoritmos meméticos

Se incluye además el ejecutable generado por `stack` para Linux Mint 18.1, con nombre `mhP3` en la carpeta `BIN`.

# Experimentos y análisis de resultados
## Descripción de los casos del problema

### Ozone

Ozone es una base de datos para detectar el nivel de ozono según una serie de atributos. Tiene 320 ejemplos de los cuales 2 eran duplicados, quedando 318 ejemplos. 

Cada ejemplo consta de 72 atributos (todos ellos continuos, que miden datos atmosféricos como la temperatura, radiación solar, velocidad del viento) y una clase que puede ser de dos tipos (concentración de ozono normal o alta).

Tras eliminar duplicados la distribución de clases está aproximadamente balanceada: hay 158 ejemplos de un tipo y 160 del otro.

### Parkinsons

Parkinsons es una base de datos utiliza para la predicción de la enfermedad de Parkinsons a partir de una serie de medidas sobre la voz de un paciente. El fichero provisto tiene 195 ejemplos sin duplicados.

Cada ejemplo consta de 23 atributos (todos ellos continuos, que miden datos de la voz como la frecuencia mínima, máxima y media, la variación de la voz o el ruido) y una clase que puede ser de dos tipos (enfermo o sano).

La distribución de clases no está balanceada: hay 147 enfermos y 48 sanos.

### Spectf-heart

Spectf-heart es una base de datos para determinar si la fisiología de un corazón es correcta según una serie de atributos. El fichero dado consta de 82 duplicados. Tras eliminarlos quedan 267 ejemplos.

Cada ejemplo tiene 44 atributos (todos ellos enteros pero continuos, que miden características sobre la fisiología del corazón a partir de tomografía computerizada) y una clase que puede ser sano o patología cardiaca.

La distribución de clases no es balanceada: hay 55 ejemplos enfermos y 212 sanos

## Resultados obtenidos

En esta sección mostramos los resultados obtenidos para cada uno de los algoritmos. 
Muestro los resultados en tablas separadas y luego en una tabla resumen siguiendo el formato indicado. 
Conservo los resultados medios de los algoritmos de prácticas anteriores.
El análisis y comentario de los resultados se realiza en [Análisis de los resultados].

Todos los resultados se han redondeado a **2 cifras significativas**. 
Cuando un resultado es 0,00 significa que su valor es menor que 0,005. 
Cada fila se corresponde con una partición (se consideran las mismas particiones para todos los algoritmos).
Las columnas de la tabla son:

Nº
: el número de la partición

Cls
: La tasa de clasificación (precisión) en puntos porcentuales.

Red
: La tasa de reducción (simplicidad) en puntos porcentuales.

Agr
: La función objetivo agregada con $\alpha = \frac12$

T
: El tiempo que se ha tardado en ejecutar el algoritmo para esa partición.

Esta ejecución particular se ha obtenido con la semilla \texttt{"317415316 1"}.
Puede ejecutarse el programa para reproducir los resultados siguiendo las instrucciones de la sección [Procedimiento considerado para desarrollar la práctica].

Los tiempos de ejecución corresponde a mi portátil con sistema operativo Linux Mint 18.1 64-bit y procesador Intel© Core™ i7-4760HQ CPU @ 2.10GHz x 4.


\begin{table}[htbp]
\caption{Resultados obtenidos por el algoritmo ILS en el problema del APC}
\begin{center}
\begin{tabular}{ccccc|cccc|cccc}
   &	\multicolumn{4}{c}{\textsc{Ozone}} & 	\multicolumn{4}{c}{\textsc{Parkinsons}} &	\multicolumn{4}{c}{\textsc{Spectf-heart}}  \\ 
\textbf{Nº} & \textbf{Cls} & \textbf{Red}  & \textbf{Agr}   & \textbf{T}    & \textbf{Cls}   & \textbf{Red}  & \textbf{Agr}   & \textbf{T}    & \textbf{Cls}   & \textbf{Red}  & \textbf{Agr}   &  \textbf{T} \\ \hline
1 & 81,25 & 83,33 & 82,29 & 199,40 & 75,00 & 86,36 & 80,68 & 16,15 & 75,93 & 81,82 & 78,87 & 93,48 \\
2 & 73,44 & 87,50 & 80,47 & 202,38 & 92,50 & 86,36 & 89,43 & 15,17 & 75,93 & 81,82 & 78,87 & 95,26 \\
3 & 76,56 & 83,33 & 79,95 & 196,89 & 75,00 & 90,91 & 82,95 & 16,14 & 74,07 & 81,82 & 77,95 & 95,91 \\
4 & 65,62 & 81,94 & 73,78 & 201,42 & 92,50 & 90,91 & 91,70 & 14,17 & 81,48 & 81,82 & 81,65 & 92,45 \\
5 & 72,58 & 84,72 & 78,65 & 204,70 & 91,43 & 90,91 & 91,17 & 16,37 & 72,55 & 86,36 & 79,46 & 96,48 \\ \hline
$\bar{x}$ & 73,89 & 84,16 & 79,03 & 200,96 & 85,29 & 89,09 & 87,19 & 15,60 & 75,99 & 82,73 & 79,36 & 94,71
\end{tabular}
\end{center}
\label{age-ca}
\end{table}


\begin{table}[htbp]
\caption{Resultados obtenidos por el algoritmo SA con esquema proporcional y $5n$ vecinos en el problema del APC}
\begin{center}
\begin{tabular}{ccccc|cccc|cccc}
   &	\multicolumn{4}{c}{\textsc{Ozone}} & 	\multicolumn{4}{c}{\textsc{Parkinsons}} &	\multicolumn{4}{c}{\textsc{Spectf-heart}}  \\ 
\textbf{Nº} & \textbf{Cls} & \textbf{Red}  & \textbf{Agr}   & \textbf{T}    & \textbf{Cls}   & \textbf{Red}  & \textbf{Agr}   & \textbf{T}    & \textbf{Cls}   & \textbf{Red}  & \textbf{Agr}   &  \textbf{T} \\ \hline
1 & 81,25 & 83,33 & 82,29 & 197,90 & 82,50 & 90,91 & 86,70 & 25,64 & 74,07 & 86,36 & 80,22 & 95,33 \\
2 & 75,00 & 83,33 & 79,17 & 199,36 & 92,50 & 90,91 & 91,70 & 24,63 & 79,63 & 84,09 & 81,86 & 95,27 \\
3 & 73,44 & 79,17 & 76,30 & 199,33 & 90,00 & 81,82 & 85,91 & 24,22 & 75,93 & 75,00 & 75,46 & 95,46 \\
4 & 70,31 & 84,72 & 77,52 & 199,44 & 82,50 & 90,91 & 86,70 & 24,06 & 83,33 & 77,27 & 80,30 & 95,78 \\
5 & 79,03 & 87,50 & 83,27 & 203,72 & 97,14 & 90,91 & 94,03 & 27,59 & 60,78 & 90,91 & 75,85 & 100,94 \\ \hline
$\bar{x}$ & 75,81 & 83,61 & 79,71 & 199,95 & 88,93 & 89,09 & 89,01 & 25,23 & 74,75 & 82,73 & 78,74 & 96,56
\end{tabular}
\end{center}
\label{age-blx}
\end{table}

\begin{table}[htbp]
\caption{Resultados obtenidos por el algoritmo SA con esquema de Cauchy y $10n$ vecinos en el problema del APC}
\begin{center}
\begin{tabular}{ccccc|cccc|cccc}
   &	\multicolumn{4}{c}{\textsc{Ozone}} & 	\multicolumn{4}{c}{\textsc{Parkinsons}} &	\multicolumn{4}{c}{\textsc{Spectf-heart}}  \\ 
\textbf{Nº} & \textbf{Cls} & \textbf{Red}  & \textbf{Agr}   & \textbf{T}    & \textbf{Cls}   & \textbf{Red}  & \textbf{Agr}   & \textbf{T}    & \textbf{Cls}   & \textbf{Red}  & \textbf{Agr}   &  \textbf{T} \\ \hline
1 & 81,25 & 62,50 & 71,88 & 206,86 & 90,00 & 77,27 & 83,64 & 24,40 & 77,78 & 56,82 & 67,30 & 101,27 \\
2 & 76,56 & 65,28 & 70,92 & 201,51 & 92,50 & 68,18 & 80,34 & 24,34 & 83,33 & 65,91 & 74,62 & 99,92 \\
3 & 79,69 & 61,11 & 70,40 & 204,05 & 90,00 & 72,73 & 81,36 & 26,33 & 70,37 & 56,82 & 63,59 & 101,49 \\
4 & 75,00 & 61,11 & 68,06 & 203,39 & 82,50 & 72,73 & 77,61 & 24,43 & 81,48 & 63,64 & 72,56 & 98,38 \\
5 & 82,26 & 62,50 & 72,38 & 205,25 & 88,57 & 77,27 & 82,92 & 26,21 & 60,78 & 61,36 & 61,07 & 101,31 \\  \hline
$\bar{x}$ & 78,95 & 62,50 & 70,73 & 204,21 & 88,71 & 73,64 & 81,17 & 25,14 & 74,75 & 60,91 & 67,83 & 100,47
\end{tabular}
\end{center}
\label{agg-ca}
\end{table}


\begin{table}[htbp]
\caption{Resultados obtenidos por el algoritmo DE/Rand/1 en el problema del APC}
\begin{center}
\begin{tabular}{ccccc|cccc|cccc}
   &	\multicolumn{4}{c}{\textsc{Ozone}} & 	\multicolumn{4}{c}{\textsc{Parkinsons}} &	\multicolumn{4}{c}{\textsc{Spectf-heart}}  \\ 
\textbf{Nº} & \textbf{Cls} & \textbf{Red}  & \textbf{Agr}   & \textbf{T}    & \textbf{Cls}   & \textbf{Red}  & \textbf{Agr}   & \textbf{T}    & \textbf{Cls}   & \textbf{Red}  & \textbf{Agr}   &  \textbf{T} \\ \hline
1 & 76,56 & 88,89 & 82,73 & 208,90 & 85,00 & 90,91 & 87,95 & 27,18 & 83,33 & 93,18 & 88,26 & 101,35 \\
2 & 79,69 & 93,06 & 86,37 & 209,36 & 95,00 & 90,91 & 92,95 & 26,20 & 77,78 & 93,18 & 85,48 & 101,67 \\
3 & 82,81 & 88,89 & 85,85 & 210,09 & 90,00 & 90,91 & 90,45 & 26,10 & 74,07 & 90,91 & 82,49 & 101,21 \\
4 & 70,31 & 90,28 & 80,30 & 208,94 & 92,50 & 90,91 & 91,70 & 26,14 & 74,07 & 93,18 & 83,63 & 100,96 \\
5 & 75,81 & 91,67 & 83,74 & 214,20 & 91,43 & 90,91 & 91,17 & 27,86 & 70,59 & 90,91 & 80,75 & 103,74 \\  \hline
$\bar{x}$ & 77,04 & 90,56 & 83,80 & 210,30 & 90,79 & 90,91 & 90,84 & 26,70 & 75,97 & 92,27 & 84,12 & 101,78
\end{tabular}
\end{center}
\label{agg-blx}
\end{table}

\begin{table}[htbp]
\caption{Resultados obtenidos por el algoritmo DE/current-to-best/1 en el problema del APC}
\begin{center}
\begin{tabular}{ccccc|cccc|cccc}
   &	\multicolumn{4}{c}{\textsc{Ozone}} & 	\multicolumn{4}{c}{\textsc{Parkinsons}} &	\multicolumn{4}{c}{\textsc{Spectf-heart}}  \\ 
\textbf{Nº} & \textbf{Cls} & \textbf{Red}  & \textbf{Agr}   & \textbf{T}    & \textbf{Cls}   & \textbf{Red}  & \textbf{Agr}   & \textbf{T}    & \textbf{Cls}   & \textbf{Red}  & \textbf{Agr}   &  \textbf{T} \\ \hline
1 & 75,00 & 68,06 & 71,53 & 203,64 & 82,50 & 90,91 & 86,70 & 25,74 & 79,63 & 77,27 & 78,45 & 98,44 \\
2 & 81,25 & 69,44 & 75,35 & 203,39 & 85,00 & 81,82 & 83,41 & 25,12 & 64,81 & 72,73 & 68,77 & 96,74 \\
3 & 85,94 & 62,50 & 74,22 & 201,76 & 90,00 & 81,82 & 85,91 & 25,04 & 70,37 & 68,18 & 69,28 & 97,83 \\
4 & 67,19 & 62,50 & 64,84 & 199,70 & 80,00 & 77,27 & 78,64 & 24,92 & 72,22 & 75,00 & 73,61 & 99,01 \\
5 & 83,87 & 75,00 & 79,44 & 203,57 & 91,43 & 90,91 & 91,17 & 27,08 & 62,75 & 77,27 & 70,01 & 102,46 \\  \hline
$\bar{x}$ & 78,65 & 67,50 & 73,08 & 202,41 & 85,79 & 84,55 & 85,17 & 25,58 & 69,96 & 74,09 & 72,02 & 98,90
\end{tabular}
\end{center}
\label{am-10-1}
\end{table}


\begin{table}[htbp]
\caption{Resultados globales en el problema del APC.En negrita la mejor tasa agregada para el dataset. En cursiva la segunda mejor.}
\vspace*{0.3cm}
\begin{adjustwidth}{-0.7in}{-0.7in}
\begin{center}
\begin{tabular}{ccccc|cccc|cccc}
   &  \multicolumn{4}{c}{\textsc{Ozone}} & \multicolumn{4}{c}{\textsc{Parkinsons}} &  \multicolumn{4}{c}{\textsc{Spectf-heart}}  \\ 
\textbf{Nombre} & \textbf{Cls} & \textbf{Red}  & \textbf{Agr}   & \textbf{T}    & \textbf{Cls}   & \textbf{Red}  & \textbf{Agr}   & \textbf{T}    & \textbf{Cls}   & \textbf{Red}  & \textbf{Agr}   &  \textbf{T} \\ \hline
CERO & 49,68 & 100 & 74,84 & 0,0032 & 75,428 & 100 & 87,71 & 0,00164 & 79,39 & 100 & 89,69 & 0,0025 \\
RAND & 77,3 & 12,5 & 44,9 & 0 & 96,43 & 9,09 & 52,76 & 0 & 68,78 & 8,3 & 38,54 & 0 \\
1-NN & 80,18 & 0,00 & 40,09 & 0,01 & 96,86 & 0,00 & 48,43 & 0 & 70,72 & 0,00 & 35,36 & 0 \\
RELIEF & 78,31 & 17,22 & 47,765 & 0,02 & 97,36 & 3,64 & 50,5 & 0 & 72,57 & 38,64 & 55,61 & 0,01 \\
BL & 74,26 & 72,50 & 73,38 & 62,29 & 86,79 & 79,09 & 82,94 & 2,02 & 72,57 & 72,27 & 72,42 & 16,16 \\ \hline
ILS & 73,89 & 84,16 & 79,03 & 200,96 & 85,29 & 89,09 & 87,19 & 15,60 & 75,99 & 82,73 & \textit{79,36} & 94,71 \\
SA(P) & 75,81 & 83,61 & \textit{79,71} & 199,95 & 88,93 & 89,09 & \textit{89,01} & 25,23 & 74,75 & 82,73 & 78,74 & 96,56 \\
SA(C) & 78,95 & 62,50 & 70,73 & 204,21 & 88,71 & 73,64 & 81,17 & 25,14 & 74,75 & 60,91 & 67,83 & 100,47 \\
DE/R & 77,04 & 90,56 & \textbf{83,80} & 210,30 & 90,79 & 90,91 & \textbf{90,84} & 26,70 & 75,97 & 92,27 & \textbf{84,12} & 101,78 \\
DE/B & 78,65 & 67,50 & 73,08 & 202,41 & 85,79 & 84,55 & 85,17 & 25,58 & 69,96 & 74,09 & 72,02 & 98,90 
\end{tabular}
\end{center}
\end{adjustwidth}
\label{global}
\end{table}

\newpage

## Análisis de los resultados

Los resultados del algoritmo de búsqueda local reiterada **ILS** son algo peores en la clasificación que el resto de algoritmos incluyendo los casos base. 
En su tasa agregada supera a los casos base pero queda penúltimo en los algoritmos considerados en esta práctica en casi todos los datasets; en el dataset *Spectf-heart* sin embargo consigue quedar segundo en la tasa agregada con el mejor tiempo.

El tiempo de ejecución de ILS es algo inferior al resto de algoritmos, sobre todo en el dataset más pequeño; esto se debe a que es un algoritmo con mayor simplicidad en cuanto a la cantidad de operaciones realizadas. Sin embargo sus tiempos se igualan en el resto de datasets.

El algoritmo original de **enfriamiento simulado** con el esquema de enfriamiento de Cauchy modificado y la generación de un máximo de $10n$ vecinos (*SA*(*P*) en la tabla) tiene unos resultados muy malos, siendo superados incluso por la búsqueda local que toma un tiempo bastante menor de ejecución. Sin embargo, si modificamos el esquema a un esquema proporcional con $\alpha = 0.95$ y un número máximo de vecinos igual a 5 veces el número de características obtenemos un algoritmo mucho más competitivo; es, de hecho, el segundo mejor algoritmo en tasa agregada aún cuando la complejidad del código es bastante baja y no tenemos que mantener una población de soluciones.

Esto se debe a que el enfriamiento del esquema de Cauchy es demasiado rápido; como vemos en la gráfica de convergencia que se encuentra en la página siguiente el algoritmo con esquema de Cauchy es incapaz de encontrar ninguna solución mejor que la que obtiene tras unos cientos de evaluaciones.

![*Gráfica de convergencia de las variantes de enfriamiento simulado (ND)*](P3/sa.pdf)

La variante de **evolución diferencial** aleatoria es la clara ganadora de este análisis.
Esta variante supera al resto de algoritmos considerados, superando incluso en tasa agregada a los algoritmos meméticos considerados en la práctica anterior en todos los datasets.
Sin embargo la variante que utiliza el mejor de la población es superada por el enfriamiento simulado y la búsqueda local reiterada. Esto nos indica que el cruce elegido puede tener grandes efectos en el rendimiento de un algoritmo.
En ambas variantes su tiempo de ejecución es ligeramente superior al del resto de algoritmos aunque no tiene grandes diferencias ya que la mayoría del tiempo se dedica en hacer las evaluaciones.

Como conclusión podemos ver que algunos de los métodos de búsquedas basadas en trayectorias pueden ofrecer los mejores resultados de entre las metaheurísticas que se han considerado en estas prácticas.
En particular la evolución diferencial ofrece los mejores resultados de todos los algoritmos considerados en estas prácticas, y es posible que otras versiones como L-SHADE o JADE ofrezcan resultados aún mejores.
