---
title: "\\textbf{Práctica 1.b:} \\ Técnicas de Búsqueda Local \\ y Algoritmos Greedy"
author: Pablo Baeyens Fernández
subtitle: Metaheurísticas
documentclass: scrbook
classoption: oneside
lang: es
algos: RELIEF, Búsqueda Local
colorlinks: true
bibliography: assets/citas.bib
biblio-style: apalike
link-citations: true
citation-style: assets/estilo.csl
numbersections: true
toc: true
---

# Descripción de los algoritmos considerados

Léase la sección [Nota sobre el pseudocódigo] para ver consideraciones comunes a todos los trozos de pseucódigo.
Para simplificar la exposición de los algoritmos uso los operadores aritméticos(`+`, `-`, `*`, ...) sobre vectores. La operación se realiza componente a componente.

## Algoritmo *greedy* RELIEF

El algoritmo *greedy* RELIEF recorre el dataset y, para cada punto, modifica el vector de pesos en función del enemigo y amigo más cercano. En primer lugar describimos algunas funciones auxiliares.

La función `normaliza`, normaliza un vector de pesos, eliminando los pesos negativos y dividiendo por el máximo

```haskell
normaliza :: Weights → Weights
normaliza ws = map (\x → if x < 0 then 0 else x/wm) ws
  where wm = máximo ws
```

La función `d1` toma dos vectores y calcula un vector con las diferencias en valor absoluto coordenada a coordenada (`abs` es la función valor absoluto):

```haskell
d1 :: Weights → Weights → Weights
d1 xs ys = map abs (xs - ys)
```

La función `masCercano` nos da el punto más cercano a uno dado en una lista de puntos. `uno` es un vector de pesos 1 (para utilizar la distancia euclídea):

```haskell
masCercano ds p = mínimoCon (compara (dist uno p)) ds
```

La función `diffs` ejecuta el bucle principal. Toma una lista de `amigos` y otra de `enemigos`. Recorre la lista de `amigos` y acumula en un vector de pesos inicialmente cero (`cero`) las diferencias en valor absoluto al amigo y enemigo mas cercano (`amigos\\p` es la lista de amigos sin `p`).

```haskell
diffs enemigos amigos = acumula delta cero amigos
  where delta p acc = acc + (d1 (enemigo p)  p) - (d1 (amigo p)  p)
        amigo   p = masCercano (amigos\\p) p
        enemigo p = masCercano  enemigos   p
```

Finalmente `relief` implementa el algoritmo RELIEF. Agrupa los datos por clase y aplica `diffs` tomando primero como `enemigos` los de una clase y luego al revés. A continuación normaliza.

```haskell
relief :: Algorithm
relief ds = normaliza ((diffs clase2 clase1) + (diffs clase1 clase2))
  where [clase1, clase2] = agrupaPor claseDe ds
```

## Algoritmo de búsqueda local

*Nota sobre la función* `vecino`: en el guión se especifica que hay que variar una componente distinta en cada paso, pero en clase de prácticas se mencionó que había que modificar una componente aleatoria. He optado por la segunda versión ya que el pseudocódigo es algo más sencillo. 
Aún así, he implementado ambas versiones y no he observado diferencias significativas en los resultados obtenidos por ambos.

***

El algoritmo de búsqueda local primero mejor utiliza la segunda representación de solución descrita en [Nota sobre el pseudocódigo] para optimizar los cálculos.
Esta solución guarda el número de solución, su evaluación y el vector de pesos, que representamos:

```haskell
Sol {nSol = n, 
     evaluación = e,
     pesos = ws}
```

En primer lugar definimos un función que dado un punto genera un vecino suyo modificando una componente del vector y sumando un número de una normal.
Las funciones `normal` y `uniforme` generan un valor aleatorio de las distribuciones normal estándar y uniforme respectivamente.
La función `modifica v i x` hace `v[i] = x`.

```haskell
vecino :: Point → Point
vecino  v = modifica v i nuevoValor
  where i = uniforme 0 (n-1)
        nuevoValor = max 0 (min 1 (v[i] + 0.3*normal))
```

A continuación definimos un paso de la ejecución de la búsqueda local. En un paso se hace exactamente una evaluación de la función objetivo
        
```haskell
paso :: DataSet → (Int, Solution) → (Int, Solution)
paso ds (n, actual) = (n+1, mejor)
  where
    vec   = vecino (pesos actual)
    nuevo = Sol {nSol = n+1, 
                 evaluación = objetivo vec ds,
                 pesos = vec}
    mejor = if evaluación nuevo > evaluación actual 
               then nuevo 
               else actual
```


El bucle principal de la búsqueda local utiliza la función `hastaQue`. Esta función es similar a un bucle `while`; es equivalente a la siguiente función en pseucódigo con estilo imperativo:

```python
def hastaQue(p, f, inicial):
  valor = inicial
  while(!p(x)):
    valor = f(valor)
  return valor
```

El bucle principal ejecuta `paso` hasta que el número de evaluaciones sea mayor que 15000 o no se haya mejorado en $20n$ pasos. Guardamos un par que indica el número de evaluaciones hasta el momento y la mejor solución obtenida:

```haskell
local :: Algorithm
local ds = pesos
  hastaQue (nEvaluaciones > min 15000 (nSol + 20*n))
           (paso ds)
           (nEvaluaciones = 0, 
            Sol {nSol = 1,
                 evaluación = objetivo inicial ds,
                 pesos = inicial})
  where
    inicial = obtenUniforme
```

## Casos de comparación

Además se han implementado también algunos casos base para el análisis de datos. No describo explícitamente los algoritmos de estos casos pero sí los listo a continuación para ayudar a la interpretación de los datos en la tabla de [Resultados obtenidos]:

- Se considera el algoritmo para el caso de la distancia euclídea, es decir, implemento un algoritmo que devuelve siempre el vector de pesos todos 1 (simplicidad mínima).
- Consideramos clasificación aleatoria: es decir, ejecutamos el algoritmo con un vector de pesos todos 0 (simplicidad máxima). La precisión dependerá de la proporción de las clases en el dataset.
- Finalmente consideramos el caso de generar un vector de pesos aleatorio (generado con una distribución uniforme sobre $[0,1]$ para cada peso) para compararlo con el algoritmo greedy. En este caso la simplicidad media esperada es 0.2 (si $X \sim U([0,1])$ tenemos que $P[X \leq \text{0.2}] = \text{0.2}$).

# Procedimiento considerado para desarrollar la práctica

La implementación de la práctica se ha realizado en el lenguaje de programación funcional puro [**Haskell**](https://www.haskell.org/).
No se ha utilizado ningún framework de metaheurísticas.

El código se incluye en la carpeta asociada. Para compilarlo es necesario instalar `stack` y *LLVM* en su versión 3.9.
La instalación de `stack` puede hacerse siguiendo las instrucciones que aparecen [en su página web](https://docs.haskellstack.org/).
`stack` se encarga de descargar los paquetes necesarios para la compilación del programa.

Para el SO que he utilizado para desarrollar la práctica (Linux Mint) *LLVM* puede instalarse con el paquete `llvm-3.9`. 
Es importante añadir al `PATH` la carpeta `/usr/lib/llvm-3.9/bin/`.
Puede consultarse la instalación de LLVM para otras plataformas [en su página web](https://llvm.org/).
Opcionalmente puede no instalarse `llvm` (con lo que el programa se compilará vía `gcc`) eliminando la flag `-fllvm` de `ghc-options` en el fichero `mhP1.cabal`.

Se incluye un Makefile para facilitar la ejecución. 
Para compilar el programa con `stack` y ejecutarlo sin argumentos hacemos:

```
make build
make run
```

La primera ejecución de `stack` necesita descargar el compilador de Haskell y los paquetes necesarios para satisfacer las dependencias por lo que es probable que tarde un tiempo.

El programa admite 3 modos de funcionamiento:

- Si no se pasa ningún argumento (`make run`) el programa ejecuta todos los algoritmos para todos los ficheros que estén en la carpeta `Data` con un generador de números aleatorio obtenido del sistema.
- Si se le pasa un único argumento lee un generador de ese argumento y ejecuta todos los algoritmos para todos los ficheros que estén en la carpeta `Data` con ese generador.
- Si se le pasa más de un argumento lee un generador del primer argumento y a continuación lee una lista de ficheros. Ejecuta todos los algoritmos sobre esos ficheros con el generador dado como argumento. 

\newpage

Para los dos últimos modos ejecutamos directamente:

```bash
./mhP1 generador [ficheros]
```

El generador utilizado para una ejecución en la que no se provee de uno se imprime al comenzar la ejecución. 
Para pasarlo como argumento debe pasarse entre comillas.

El programa **asume que los saltos de línea son de tipo UNIX** (es decir, los finales de línea deben ser `\n` y no `\r\n`) y elimina datos duplicados de los ficheros de entrada.
Se incluye una carpeta `Data` con los ficheros de entrada de los datasets a analizar en la práctica con saltos de línea tipo UNIX.

El código está disponible en la carpeta `FUENTES` y se divide en las siguientes partes:

- `Main.hs` y `Base.hs` define el bucle principal de ejecución del programa y los tipos básicos
- `Read.hs` implementa la lectura de archivos tipo arff.
- `KNN.hs` implementa el algoritmo de vecino más cercano y las funciones de evaluación de un vector de pesos (*leave one out*, tasa de clasificación, tasa de simplicidad...).
- `APC.hs` implementa los algoritmos de aprendizaje de pesos en características y funciones auxiliares para los mismos.

Se incluye además el ejecutable generado por `stack` para Linux Mint 18.1, con nombre `mhP1` en la carpeta `BIN`.

# Experimentos y análisis de resultados

## Descripción de los casos del problema

He considerado los 3 casos del problema para los que se me proporcionaba datos. 
En todos los casos he filtrado duplicados.

### Ozone

Ozone es una base de datos para detectar el nivel de ozono según una serie de atributos. El fichero provisto tiene 320 ejemplos de los cuales 2 eran duplicados, quedando de forma efectiva 318 ejemplos. 

Cada ejemplo consta de 72 atributos (todos ellos continuos, que miden datos atmosféricos como la temperatura, radiación solar, velocidad del viento) y una clase que puede ser de dos tipos (concentración de ozono normal o alta).

Tras eliminar duplicados la distribución de clases está aproximadamente balanceada: hay 158 ejemplos de un tipo y 160 del otro.

### Parkinsons

Parkinsons es una base de datos utiliza para la predicción de la enfermedad de Parkinsons a partir de una serie de medidas sobre la voz de un paciente. El fichero provisto tiene 195 ejemplos sin duplicados.

Cada ejemplo consta de 23 atributos (todos ellos continuos, que miden datos de la voz como la frecuencia mínima, máxima y media, la variación de la voz o el ruido) y una clase que puede ser de dos tipos (enfermo o sano).

La distribución de clases no está balanceada: hay 147 enfermos y 48 sanos.

### Spectf-heart

Spectf-heart es una base de datos para determinar si la fisiología de un corazón es correcta según una serie de atributos. El fichero dado consta de 82 duplicados. Tras eliminarlos quedan 267 ejemplos.

Cada ejemplo tiene 44 atributos (todos ellos enteros pero continuos, que miden características sobre la fisiología del corazón a partir de tomografía computerizada) y una clase que puede ser sano o patología cardiaca.

La distribución de clases no es balanceada: hay 55 ejemplos enfermos y 212 sanos

## Resultados obtenidos

En esta sección mostramos los resultados obtenidos para cada uno de los algoritmos. 
Muestro los resultados en tablas separadas y luego en una tabla resumen siguiendo el formato indicado. 
El análisis y comentario de los resultados se realiza en [Análisis de los resultados].

Todos los resultados se han redondeado a **2 cifras significativas**. 
Cuando un resultado es 0,00 significa que su valor es menor que 0,005. 
Cada fila se corresponde con una partición (se consideran las mismas particiones para todos los algoritmos).
Las columnas de la tabla son:

Nº
: el número de la partición

Cls
: La tasa de clasificación (precisión) en puntos porcentuales.

Red
: La tasa de reducción (simplicidad) en puntos porcentuales.

Agr
: La función objetivo agregada con $\alpha = \frac12$

T
: El tiempo que se ha tardado en ejecutar el algoritmo para esa partición.

Esta ejecución particular se ha obtenido con el generador de números aleatorios \texttt{"317415316 1"}.
Puede ejecutarse el programa para reproducir los resultados siguiendo las instrucciones de la sección [Procedimiento considerado para desarrollar la práctica].

Los tiempos de ejecución corresponde a mi portátil con sistema operativo Linux Mint 18.1 64-bit y procesador Intel© Core™ i7-4760HQ CPU @ 2.10GHz x 4.


En primer lugar mostramos en \ref{cero} los resultados para el vector de pesos todos 0 como caso para comparación.


\begin{table}[htbp]
\caption{Resultados obtenidos por el algoritmo CERO en el problema del APC}
\begin{center}
\begin{tabular}{ccccc|cccc|cccc}
   &	\multicolumn{4}{c}{\textsc{Ozone}} & 	\multicolumn{4}{c}{\textsc{Parkinsons}} &	\multicolumn{4}{c}{\textsc{Spectf-heart}}  \\ 
\textbf{Nº} & \textbf{Cls} & \textbf{Red}  & \textbf{Agr}   & \textbf{T}    & \textbf{Cls}   & \textbf{Red}  & \textbf{Agr}   & \textbf{T}    & \textbf{Cls}   & \textbf{Red}  & \textbf{Agr}   &  \textbf{T} \\ \hline
1 & 50,00 & 100,00 & 75,00 & 0,00 & 75,00 & 100,00 & 87,50 & 0,01 & 79,63 & 100,00 & 89,81 & 0,01 \\
2 & 50,00 & 100,00 & 75,00 & 0,00 & 75,00 & 100,00 & 87,50 & 0,00 & 79,63 & 100,00 & 89,81 & 0,00 \\
3 & 50,00 & 100,00 & 75,00 & 0,00 & 75,00 & 100,00 & 87,50 & 0,00 & 79,63 & 100,00 & 89,81 & 0,00 \\
4 & 50,00 & 100,00 & 75,00 & 0,00 & 75,00 & 100,00 & 87,50 & 0,00 & 79,63 & 100,00 & 89,81 & 0,00 \\
5 & 48,39 & 100,00 & 74,20 & 0,00 & 77,14 & 100,00 & 88,57 & 0,00 & 78,43 & 100,00 & 89,22 & 0,00 \\ \hline
$\bar{x}$ & 49,68 & 100,00 & 74,84 & 0,00 & 75,43 & 100,00 & 87,71 & 0,00 & 79,39 & 100,00 & 89,69 & 0,00
\end{tabular}
\end{center}
\label{cero}
\end{table}

En segundo lugar también como caso para comparación mostramos los resultados con un mismo vector de pesos aleatorio para todas las particiones en \ref{random}.

\begin{table}[htbp]
\caption{Resultados obtenidos por el algoritmo RANDOM en el problema del APC}
\begin{center}
\begin{tabular}{ccccc|cccc|cccc}
   &	\multicolumn{4}{c}{\textsc{Ozone}} & 	\multicolumn{4}{c}{\textsc{Parkinsons}} &	\multicolumn{4}{c}{\textsc{Spectf-heart}}  \\ 
\textbf{Nº} & \textbf{Cls} & \textbf{Red}  & \textbf{Agr}   & \textbf{T}    & \textbf{Cls}   & \textbf{Red}  & \textbf{Agr}   & \textbf{T}    & \textbf{Cls}   & \textbf{Red}  & \textbf{Agr}   &  \textbf{T} \\ \hline
1 & 79,69 & 12,50 & 46,10 & 0,00 & 95,00 & 9,09 & 52,05 & 0,00 & 70,37 & 8,30 & 39,34 & 0,00 \\ 
2 & 81,25 & 12,50 & 46,88 & 0,00 & 97,50 & 9,09 & 53,30 & 0,00 & 72,22 & 8,30 & 40,26 & 0,00 \\ 
3 & 76,56 & 12,50 & 44,53 & 0,00 & 97,50 & 9,09 & 53,30 & 0,00 & 70,37 & 8,30 & 39,34 & 0,00 \\ 
4 & 70,31 & 12,50 & 41,41 & 0,00 & 95,00 & 9,09 & 52,05 & 0,00 & 74,07 & 8,30 & 41,19 & 0,00 \\ 
5 & 79,03 & 12,50 & 45,77 & 0,00 & 97,14 & 9,09 & 53,12 & 0,00 & 56,86 & 8,30 & 32,58 & 0,00 \\ \hline
$\bar{x}$ & 77,37 & 12,50 & 44,93 & 0,00 & 96,43 & 9,09 & 52,76 & 0,00 & 68,78 & 8,30 & 38,54 & 0,00
\end{tabular}
\end{center}
\label{random}
\end{table}

A continuación mostramos los resultados para el vector de pesos 1 (que llamamos 1-NN) en la tabla \ref{uno}.

\begin{table}[htbp]
\caption{Resultados obtenidos por el algoritmo 1-NN en el problema del APC}
\begin{center}
\begin{tabular}{ccccc|cccc|cccc}
   &	\multicolumn{4}{c}{\textsc{Ozone}} & 	\multicolumn{4}{c}{\textsc{Parkinsons}} &	\multicolumn{4}{c}{\textsc{Spectf-heart}}  \\ 
\textbf{Nº} & \textbf{Cls} & \textbf{Red}  & \textbf{Agr}   & \textbf{T}    & \textbf{Cls}   & \textbf{Red}  & \textbf{Agr}   & \textbf{T}    & \textbf{Cls}   & \textbf{Red}  & \textbf{Agr}   &  \textbf{T} \\ \hline
1 & 82,81 & 0,00 & 41,41 & 0,02 & 95,00 & 0,00 & 47,50 & 0,01 & 68,52 & 0,00 & 34,26 & 0,01 \\
2 & 81,25 & 0,00 & 40,63 & 0,00 & 97,50 & 0,00 & 48,75 & 0,00 & 75,93 & 0,00 & 37,97 & 0,00 \\
3 & 82,81 & 0,00 & 41,41 & 0,00 & 97,50 & 0,00 & 48,75 & 0,00 & 66,67 & 0,00 & 33,34 & 0,00 \\
4 & 75,00 & 0,00 & 37,50 & 0,00 & 100,00 & 0,00 & 50,00 & 0,00 & 77,78 & 0,00 & 38,89 & 0,00 \\
5 & 79,03 & 0,00 & 39,52 & 0,00 & 94,29 & 0,00 & 47,15 & 0,00 & 64,71 & 0,00 & 32,36 & 0,00 \\ \hline
$\bar{x}$ & 80,18 & 0,00 & 40,09 & 0,01 & 96,86 & 0,00 & 48,43 & 0,00 & 70,72 & 0,00 & 35,36 & 0,00 
\end{tabular}
\end{center}
\label{uno}
\end{table}

Por último mostramos los resultados de los dos algoritmos. Los resultados del algoritmo *greedy* RELIEF pueden verse en \ref{relief}.

\begin{table}[htbp]
\caption{Resultados obtenidos por el algoritmo RELIEF en el problema del APC}
\begin{center}
\begin{tabular}{ccccc|cccc|cccc}
   &	\multicolumn{4}{c}{\textsc{Ozone}} & 	\multicolumn{4}{c}{\textsc{Parkinsons}} &	\multicolumn{4}{c}{\textsc{Spectf-heart}}  \\ 
\textbf{Nº} & \textbf{Cls} & \textbf{Red}  & \textbf{Agr}   & \textbf{T}    & \textbf{Cls}   & \textbf{Red}  & \textbf{Agr}   & \textbf{T}    & \textbf{Cls}   & \textbf{Red}  & \textbf{Agr}   &  \textbf{T} \\ \hline
1 & 82,81 & 9,72 & 46,27 & 0,02 & 95,00 & 4,55 & 49,78 & 0,00 & 77,78 & 43,18 & 60,48 & 0,01 \\ 
2 & 79,69 & 13,89 & 46,79 & 0,02 & 100,00 & 4,55 & 52,28 & 0,00 & 79,63 & 36,36 & 58,00 & 0,01 \\ 
3 & 81,25 & 13,89 & 47,57 & 0,02 & 97,50 & 0,00 & 48,75 & 0,00 & 66,67 & 40,91 & 53,79 & 0,01 \\ 
4 & 68,75 & 20,83 & 44,79 & 0,02 & 100,00 & 4,55 & 52,28 & 0,00 & 74,07 & 34,09 & 54,08 & 0,01 \\ 
5 & 79,03 & 27,78 & 53,41 & 0,02 & 94,29 & 4,55 & 49,42 & 0,00 & 64,71 & 38,64 & 51,68 & 0,01 \\ \hline
$\bar{x}$ & 78,31 & 17,22 & 47,76 & 0,02 & 97,36 & 3,64 & 50,50 & 0,00 & 72,57 & 38,64 & 55,60 & 0,01
\end{tabular}
\end{center}
\label{relief}
\end{table}

Los resultados de la búsqueda local primero mejor puede verse en \ref{local}.

\begin{table}[htbp]
\caption{Resultados obtenidos por el algoritmo BL en el problema del APC}
\begin{center}
\begin{tabular}{ccccc|cccc|cccc}
   &	\multicolumn{4}{c}{\textsc{Ozone}} & 	\multicolumn{4}{c}{\textsc{Parkinsons}} &	\multicolumn{4}{c}{\textsc{Spectf-heart}}  \\ 
\textbf{Nº} & \textbf{Cls} & \textbf{Red}  & \textbf{Agr}   & \textbf{T}    & \textbf{Cls}   & \textbf{Red}  & \textbf{Agr}   & \textbf{T}    & \textbf{Cls}   & \textbf{Red}  & \textbf{Agr}   &  \textbf{T} \\ \hline
1 & 81,25 & 68,06 & 74,66 & 45,78 & 82,50 & 81,82 & 82,16 & 1,71 & 74,07 & 70,45 & 72,26 & 20,41 \\
2 & 71,88 & 73,61 & 72,75 & 80,99 & 82,50 & 72,73 & 77,61 & 1,54 & 74,07 & 59,09 & 66,58 & 10,67 \\
3 & 73,44 & 72,22 & 72,83 & 61,55 & 82,50 & 77,27 & 79,89 & 2,55 & 72,22 & 79,55 & 75,88 & 16,44 \\
4 & 64,06 & 69,44 & 66,75 & 45,59 & 95,00 & 86,36 & 90,68 & 2,28 & 77,78 & 79,55 & 78,66 & 17,98 \\
5 & 80,65 & 79,17 & 79,91 & 77,54 & 91,43 & 77,27 & 84,35 & 2,03 & 64,71 & 72,73 & 68,72 & 15,29 \\ \hline
$\bar{x}$ & 74,26 & 72,50 & 73,38 & 62,29 & 86,79 & 79,09 & 82,94 & 2,02 & 72,57 & 72,27 & 72,42 & 16,16
\end{tabular}
\end{center}
\label{local}
\end{table}


La siguiente tabla comparativa (\ref{global}) muestra una comparativa de los tiempos medios de las tablas anteriores.

\begin{table}[htbp]
\caption{Resultados globales en el problema del APC}
\vspace*{0.3cm}
\begin{adjustwidth}{-0.5in}{-0.5in}
\begin{center}
\begin{tabular}{ccccc|cccc|cccc}
   &  \multicolumn{4}{c}{\textsc{Ozone}} & \multicolumn{4}{c}{\textsc{Parkinsons}} &  \multicolumn{4}{c}{\textsc{Spectf-heart}}  \\ 
\textbf{Nombre} & \textbf{Cls} & \textbf{Red}  & \textbf{Agr}   & \textbf{T}    & \textbf{Cls}   & \textbf{Red}  & \textbf{Agr}   & \textbf{T}    & \textbf{Cls}   & \textbf{Red}  & \textbf{Agr}   &  \textbf{T} \\ \hline
CERO & 49,68 & 100 & 74,84 & 0 & 75,43 & 100 & 87,71 & 0,0 & 79,39 & 100 & 89,69 & 0 \\
RANDOM & 77,3 & 12,5 & 44,9 & 0 & 96,43 & 9,09 & 52,76 & 0 & 68,78 & 8,3 & 38,54 & 0 \\ 
1-NN & 80,18 & 0,00 & 40,09 & 0,01 & 96,86 & 0,00 & 48,43 & 0 & 70,72 & 0,00 & 35,36 & 0 \\ 
RELIEF & 78,31 & 17,22 & 47,765 & 0,02 & 97,36 & 3,64 & 50,5 & 0 & 72,57 & 38,64 & 55,605 & 0,01 \\
BL     & 74,26 & 72,50 & 73,38 & 62,29 & 86,79 & 79,09 & 82,94 & 2,02 & 72,57 & 72,27 & 72,42 & 16,16
\end{tabular}
\end{center}
\end{adjustwidth}
\label{global}
\end{table}


## Análisis de los resultados

### Casos base

Considero de forma separada en primer lugar los casos base para la comparación: el vector de pesos nulo, el vector de pesos uno y un vector de pesos aleatorio.

El algoritmo **cero** (\ref{cero}) devuelve un vector de pesos todos nulos, por lo que tiene *simplicidad* máxima. 
La *clasificación* es aleatoria: la distancia en este caso es siempre nula y por tanto se toma el primer vector de la partición, que es aleatorio. 
La tasa de clasificación esperada es siempre mayor que el 50% en función de cómo de equilibradas estén las clases; por ejemplo, en el caso de Ozone podemos ver que la tasa de clasificación es de 50,00 ya que las clases están aproximadamente equilibradas en este dataset. El *tiempo de ejecución* es de menos de una centésima ya que es constante.

Aunque en este ejemplo aporta resultados *agregados* con puntuación alta (la simplicidad es máxima por tanto la puntuación agregada es al menos del 50%) esta puntuación depende de la distribución de los datos en el dataset y por tanto es muy probable que sea inútil cuando intentemos clasificar un ejemplo que no esté en nuestra base de datos.
Por ejemplo, si intentamos predecir la enfermedad de Parkinsons la proporción enfermo-sanos estará mucho más sesgada que en este dataset.
Lo incluyo para realizar comparaciones y para valorar la función objetivo.

El vector de pesos **aleatorio** (\ref{random}) es otro ejemplo en el que el *tiempo de ejecución* es de menos de una centésima.
La *simplicidad* es baja en todos las ejecuciones: la simplicidad media esperada es de 0.2 ya que sólo un 20% de los pesos elegidos de forma uniforme quedarán por debajo del umbral escogido.
La *clasificación* es mejor que la aleatoria (del caso cero) y bastante buena en el caso del dataset Parkinsons (como ocurre por otra parte para cualquier algoritmo). Es difícil considerar cuál es la clasificación media esperada para un vector de pesos aleatorio pero en la mayoría de los casos será similar al 1-NN.

El clasificador **1-NN** (\ref{uno}) utiliza un vector de pesos todos 1, por lo que tiene *simplicidad* mínima.
La *clasificación* es bastante mejor que el azar (caso cero) en todos los datasets. En el datasets Parkinsons se acerca a clasificar correctamente todos los datos, lo que nos indica que las características han sido elegidas correctamente. Sin embargo, la tasa *agregada* es bastante baja porque no tenemos ninguna simplicidad. 
El *tiempo de ejecución* es de menos de una centésima en todos los datasets salvo Ozone (ya que este es el dataset más grande).

El hecho de que el clasificador *1-NN* se mejor que el vector de pesos cero justifica la existencia de este algoritmo: funciona mejor que el azar. El vector de pesos aleatorios es ligeramente mejor ya que la simplicidad es mayor, pero esto podría no ser así para otros vectores aleatorios escogidos.

### Algoritmos considerados en la práctica

En esta sección considero los algoritmos implementados en la práctica: el algoritmo *greedy* RELIEF y el algoritmo de búsqueda local (primero mejor). Comparo además estos algoritmos con los casos base.

Los resultados del algoritmo *greedy* **RELIEF** (\ref{relief}) son bastante buenos respecto a la *clasificación*, superando en algunos casos al 1-NN y en todos los casos a la clasificación aleatoria (cero). 
La *simplicidad* y la tasa *agregada* en cambio varía según el dataset:

- En [Ozone] la simplicidad es mejor que la simplicidad aleatoria pero es baja (de media 17,22). La tasa agregada supera a los casos base salvo a la clasificación aleatoria.
- En [Parkinsons] la simplicidad es peor que la solución aleatoria: el algoritmo no simplifica el vector de pesos.
  La tasa agregada es también peor que la aleatoria.
- En [Spectf-heart] la simplicidad es media-alta lo que influye en la tasa agregada
  
Al ser un algoritmo *greedy* es probable que no de la mejor solución.
En este conjunto de datasets podemos observar que es así: la búsqueda local supera a este algoritmo en todos los caso.
Sin embargo el tiempo de ejecución ronda las centésimas de segundo lo que lo hace un algoritmo muy rápido y que podemos probar como estrategia inicial.

La **búsqueda local** (\ref{local}) es el algoritmo más costoso en *tiempo de ejecución* de los considerados. 
En esta ejecución concreta del algoritmo tarda aproximadamente un minuto en cada partición del dataset más grande (Ozone).
El tiempo medio suele ser menor que el de esta ejecución (realizando otras ejecuciones el tiempo medio obtenido son 45 segundos). La desviación típica es de 13,58 segundos para Ozone, 0.32 segundos para Parkinsons y 2,54 segundos para Spectf-Heart, por lo que el tiempo de ejecución es relativamente estable. La variación es mayor cuanto más grande sea el dataset.

Es posible que la implementación pudiera optimizarse en tiempo a costa de un mayor uso de espacio memoizando la función distancia (es la función más utilizada) o parte de ella, pero aún así seguiría siendo el más costoso en tiempo. 

La *clasificación* es similar a la de RELIEF y los casos base en todos los datasets, siendo algo peor que RELIEF en dos de los datasets. 
La *simplicidad* en cambio es significativamente mayor que la del resto de algoritmos, es decir, la búsqueda local es capaz de mantener una buena tasa de clasificación aumentando la simplicidad. 
Estos dos hechos hacen que la *tasa agregada* sea también bastante mayor que otros algoritmos, superando incluso a la clasificación aleatoria en dos de los dataset (Parkinsons y Spectf-heart).

Esto nos muestra que el algoritmo de búsqueda local es capaz de buscar con mucha más precisión en el espacio de soluciones, obteniendo una convergencia rápida hacia un máximo local. 
Como ejemplo incluyo una gráfica de convergencia de la búsqueda local limitada a los 100 primeros pasos.
El eje X indica el paso en el que está y el eje Y indica la tasa agregada de la mejor solución encontrada hasta el momento. Como vemos el crecimiento puede tener periodos de estancamiento pero consigue mejoras importantes (20 puntos porcentuales) en pocos pasos.


![Gráfica de convergencia de los 100 primeros pasos de la búsqueda local sobre una de las particiones de Parkinsons (ND)](P1/Convergencia.png)


Como conclusión y mirando la comparación de resultados globales (\ref{global}) podemos apreciar los siguientes hechos:

- La **función objetivo** da un alto peso a la simplicidad del vector de pesos, lo que hace que soluciones triviales como un vector de pesos 0 que supone una clasificación aleatoria (con probabilidades ponderadas por la proporción de las clases en el dataset) tenga una tasa agregada competitiva con otros algoritmos e incluso superior a la búsqueda local en dos casos. Esto nos indica que es posible que la función objetivo no mida correctamente la calidad de una solución y sea necesario modificarla (por ejemplo modificando el parámetro $\alpha$).
- El algoritmo **RELIEF** es competitivo en precisión con la búsqueda local teniendo un tiempo de ejecución mucho menor. Sin embargo, en los datasets considerados no obtiene buena simplicidad.
- La **búsqueda local** es capaz de mantener la tasa de clasificación y aumentar la tasa de simplicidad, aumentando así la tasa agregada. Su tiempo de ejecución es elevado en comparación con el algoritmo *greedy* aunque variable en función de las condiciones iniciales y el vecino obtenido (que depende de factores aleatorios). Sería interesante explorar otros tipos de búsqueda local o variar los parámetros (como la desviación típica de la distribución normal que genera los vecinos o la solución inicial que se considera). Podemos incluso considerar como vector inicial el obtenido por el algoritmo RELIEF, lo que en los tests que he realizado ha dado incluso mejores resultados sin un coste de tiempo mayor.
- La distribución de los datos en el dataset puede hacer que podamos encontrar soluciones que exploten características no presentes en los datos generales de este problema (es decir, podemos sobreajustar la solución), como ocurre en el caso del vector de pesos 0.






